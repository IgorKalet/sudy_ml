{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3: Деревья решений. Случайный лес\n",
    "\n",
    "Правила:\n",
    "\n",
    "- Домашнее задание оценивается в 10 баллов (+ бонусные баллы).\n",
    "\n",
    "\n",
    "- Можно использовать любые свободные источники с обязательным указанием ссылки на них.\n",
    "\n",
    "\n",
    "- Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: Основы построения решающие дерева (1.5 балла)\n",
    "\n",
    "В этой части все расчёты необходимо реализовывать в виде запрограммированных формул, например, на `numpy`. **Нельзя использовать готовые реализации**. Например, если в задании требуется рассчитать энтропию, то требуется в каком-то виде релизовать расчёт по формуле, но нельзя использовать готовую реализацию `some_module.entropy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.1 (0.5 балла)** Пусть известно, что в вершину решающего дерева попали 10 объектов, 8 из которых имеют метку класса $k_1$, а 2 имеют метку класса $k_2$. Рассчитайте энтропию такого распределения классов (с натуральным логарифмом). Ответ округлите до двух знаков после запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_entropy(class_counts: dict) -> float:\n",
    "    total_samples = sum(class_counts.values())\n",
    "\n",
    "    if total_samples == 0:\n",
    "        return 0.0\n",
    "\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in class_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy += probability * math.log2(probability)\n",
    "\n",
    "    return -entropy\n",
    "\n",
    "node_data = {'k1': 8, 'k2': 2}\n",
    "entropy_value = calculate_entropy(node_data)\n",
    "\n",
    "print(f\"Энтропия: {entropy_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.2 (0.5 балла)** Пусть дополнительно известно, что вершина из предыдущего задания не является листовой и возможно такое разбиение, что в левое поддерево попадут все объекты класса $k_1$, а в правое - класса $k_2$. Посчитайте критерий информативности:\n",
    "\n",
    "$$\n",
    "Q(R_m, j, t) = H(R_m) - \\frac{|R_\\ell|}{|R_m|}H(R_\\ell) - \\frac{|R_r|}{|R_m|}H(R_r),\n",
    "$$\n",
    "\n",
    "где $R_m$ - множество объектов в разбиваемой вершине, $j$ - номер признака, по которому происходит разбиение, $t$ - порог разбиения, $R_\\ell$ - множество объектов в левом поддереве, $R_r$ - множество объектов в правом поддереве.\n",
    "\n",
    "Теперь в качестве $H(R)$ будем использовать индекс Джини:\n",
    "\n",
    "$$\n",
    "H(R) = \\sum_{k=1}^J p_k(1-p_k),\n",
    "$$\n",
    "где $J$ – общее количество классов (в нашем случае, $J = 2$).\n",
    "\n",
    "Ответ округлите до двух знаков после запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(class_counts: dict) -> float:\n",
    "    total_samples = sum(class_counts.values())\n",
    "\n",
    "    if total_samples == 0:\n",
    "        return 0.0\n",
    "\n",
    "    gini = 0.0\n",
    "\n",
    "    for count in class_counts.values():\n",
    "        if total_samples > 0:\n",
    "            probability = count / total_samples\n",
    "            gini += probability * (1 - probability)\n",
    "\n",
    "    return gini\n",
    "\n",
    "def calculate_information_gain_gini(parent_counts: dict, left_counts: dict, right_counts: dict) -> float:\n",
    "    n_parent = sum(parent_counts.values())\n",
    "    n_left = sum(left_counts.values())\n",
    "    n_right = sum(right_counts.values())\n",
    "\n",
    "    if n_parent == 0:\n",
    "        return 0.0\n",
    "\n",
    "    gini_parent = calculate_gini(parent_counts)\n",
    "    gini_left = calculate_gini(left_counts)\n",
    "    gini_right = calculate_gini(right_counts)\n",
    "\n",
    "    weighted_gini_children = (n_left / n_parent) * gini_left + (n_right / n_parent) * gini_right\n",
    "    information_gain = gini_parent - weighted_gini_children\n",
    "\n",
    "    return information_gain\n",
    "\n",
    "parent_node_counts = {'k1': 8, 'k2': 2}\n",
    "left_node_counts = {'k1': 8}\n",
    "right_node_counts = {'k2': 2}\n",
    "\n",
    "gain_value = calculate_information_gain_gini(parent_node_counts, left_node_counts, right_node_counts)\n",
    "\n",
    "print(f\"Критерий информативности: {gain_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.3 (0.5 балла)** Пусть при построении дерева образовалась листовая вершина с 10 объектами, значения целевой переменной для которых следующие: [1, 10, 5, 18, 100, 30, 50, 61, 84, 47] (решается задача регрессии). Чему будут равны предсказания модели для этих объектов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regression_leaf(target_values: list) -> float:\n",
    "    if not target_values:\n",
    "        return 0.0\n",
    "\n",
    "    total_sum = sum(target_values)\n",
    "    count = len(target_values)\n",
    "\n",
    "    return total_sum / count\n",
    "\n",
    "leaf_target_values = [1, 10, 5, 18, 100, 30, 50, 61, 84, 47]\n",
    "prediction = predict_regression_leaf(leaf_target_values)\n",
    "\n",
    "print(f\"Предсказание модели: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Решающие деревья (4.5 балла)\n",
    "\n",
    "В этой части мы напишем и протестируем собственную реализацию решающего дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.1 (1.5 балла)** Реализуйте функцию `find_best_split()`, которая должна находить оптимальное разбиение подмножества обучающей выборки в соответствии с информационным критерием из **Задания 1.2**. В качестве меры хаотичности $H(R)$ для задачи регрессии испольуйте дисперсию подвыборки (формула в ноутбуке семинара), а для задачи классификации – критерий Джини (определён в том же задании).\n",
    "\n",
    "Для категориальных признаков применяется наивный алгоритм разбиения: мы пытаемся найти одно значение, разбиение по которому сильнее всего увеличит критерий информативности. Иными словами, объекты с конкретным значением признака отправляем в левое поддерево, остальные - в правое. Обратите внимание, что это далеко не оптимальные способ учёта категориальных признаков. Например, можно было бы на каждое значение категориального признака создавать отдельное поддерево или использовать более сложные подходы.\n",
    "\n",
    "\n",
    "**Бонус:** Разрешается делать цикл для перебора порогов, но возможна имплементация без него. За имплементацию без цикла – **бонус 1 балл**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(\n",
    "    feature_vector: Union[np.ndarray, pd.DataFrame],\n",
    "    target_vector: Union[np.ndarray, pd.Series],\n",
    "    task: str = \"classification\",\n",
    "    feature_type: str = \"real\"\n",
    ") -> Tuple[np.ndarray, np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Находит оптимальное разбиение по признаку.\n",
    "\n",
    "    :param feature_vector: вектор значений признака\n",
    "    :param target_vector: вектор целевых значений\n",
    "    :param task: 'classification' или 'regression'\n",
    "    :param feature_type: 'real' или 'categorical'\n",
    "\n",
    "    :return thresholds: отсортированные пороги\n",
    "    :return ginis: значения критерия для каждого порога\n",
    "    :return threshold_best: оптимальный порог\n",
    "    :return gini_best: оптимальное значение критерия\n",
    "    \"\"\"\n",
    "\n",
    "    feature_vector = np.asarray(feature_vector).flatten()\n",
    "    target_vector = np.asarray(target_vector).flatten()\n",
    "    n_samples = len(feature_vector)\n",
    "\n",
    "    # Вычисляем критерий хаотичности для родительского узла\n",
    "    if task == \"classification\":\n",
    "        # Джини для родителя\n",
    "        unique_classes, class_counts = np.unique(target_vector, return_counts=True)\n",
    "        p = class_counts / n_samples\n",
    "        parent_criterion = np.sum(p * (1 - p))\n",
    "    else:\n",
    "        # Дисперсия для родителя\n",
    "        parent_criterion = np.var(target_vector)\n",
    "\n",
    "    if feature_type == \"real\":\n",
    "        # Для вещественных признаков ищем пороги между соседними значениями\n",
    "        sorted_indices = np.argsort(feature_vector)\n",
    "        sorted_feature = feature_vector[sorted_indices]\n",
    "        sorted_target = target_vector[sorted_indices]\n",
    "\n",
    "        # Уникальные значения для определения порогов\n",
    "        unique_values = np.unique(sorted_feature)\n",
    "\n",
    "        if len(unique_values) <= 1:\n",
    "            # Константный признак\n",
    "            return np.array([]), np.array([]), np.inf, -np.inf\n",
    "\n",
    "        # Пороги - середины между соседними уникальными значениями\n",
    "        thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "\n",
    "        # Векторизованное вычисление критерия для каждого порога\n",
    "        ginis = np.zeros(len(thresholds))\n",
    "\n",
    "        for idx, threshold in enumerate(thresholds):\n",
    "            left_mask = feature_vector <= threshold\n",
    "            right_mask = ~left_mask\n",
    "\n",
    "            n_left = np.sum(left_mask)\n",
    "            n_right = np.sum(right_mask)\n",
    "\n",
    "            # Пропускаем пороги, которые приводят к пустым поддеревьям\n",
    "            if n_left == 0 or n_right == 0:\n",
    "                ginis[idx] = -np.inf\n",
    "                continue\n",
    "\n",
    "            if task == \"classification\":\n",
    "                # Джини для левого поддерева\n",
    "                left_target = target_vector[left_mask]\n",
    "                unique_left, counts_left = np.unique(left_target, return_counts=True)\n",
    "                p_left = counts_left / n_left\n",
    "                gini_left = np.sum(p_left * (1 - p_left))\n",
    "\n",
    "                # Джини для правого поддерева\n",
    "                right_target = target_vector[right_mask]\n",
    "                unique_right, counts_right = np.unique(right_target, return_counts=True)\n",
    "                p_right = counts_right / n_right\n",
    "                gini_right = np.sum(p_right * (1 - p_right))\n",
    "\n",
    "                # Информативность (прирост Джини)\n",
    "                ginis[idx] = parent_criterion - (n_left / n_samples) * gini_left - (n_right / n_samples) * gini_right\n",
    "            else:\n",
    "                # Дисперсия для регрессии\n",
    "                left_target = target_vector[left_mask]\n",
    "                right_target = target_vector[right_mask]\n",
    "                var_left = np.var(left_target)\n",
    "                var_right = np.var(right_target)\n",
    "\n",
    "                # Информативность (прирост в уменьшении дисперсии)\n",
    "                ginis[idx] = parent_criterion - (n_left / n_samples) * var_left - (n_right / n_samples) * var_right\n",
    "\n",
    "    else:  # categorical\n",
    "        # Для категориальных признаков ищем лучшую категорию для левого поддерева\n",
    "        unique_categories = np.unique(feature_vector)\n",
    "\n",
    "        if len(unique_categories) <= 1:\n",
    "            return np.array([]), np.array([]), np.inf, -np.inf\n",
    "\n",
    "        thresholds = unique_categories\n",
    "        ginis = np.zeros(len(thresholds))\n",
    "\n",
    "        for idx, category in enumerate(thresholds):\n",
    "            left_mask = feature_vector == category\n",
    "            right_mask = ~left_mask\n",
    "\n",
    "            n_left = np.sum(left_mask)\n",
    "            n_right = np.sum(right_mask)\n",
    "\n",
    "            # Пропускаем пороги, которые приводят к пустым поддеревьям\n",
    "            if n_left == 0 or n_right == 0:\n",
    "                ginis[idx] = -np.inf\n",
    "                continue\n",
    "\n",
    "            if task == \"classification\":\n",
    "                # Джини для левого поддерева\n",
    "                left_target = target_vector[left_mask]\n",
    "                unique_left, counts_left = np.unique(left_target, return_counts=True)\n",
    "                p_left = counts_left / n_left\n",
    "                gini_left = np.sum(p_left * (1 - p_left))\n",
    "\n",
    "                # Джини для правого поддерева\n",
    "                right_target = target_vector[right_mask]\n",
    "                unique_right, counts_right = np.unique(right_target, return_counts=True)\n",
    "                p_right = counts_right / n_right\n",
    "                gini_right = np.sum(p_right * (1 - p_right))\n",
    "\n",
    "                # Информативность\n",
    "                ginis[idx] = parent_criterion - (n_left / n_samples) * gini_left - (n_right / n_samples) * gini_right\n",
    "            else:\n",
    "                # Дисперсия для регрессии\n",
    "                left_target = target_vector[left_mask]\n",
    "                right_target = target_vector[right_mask]\n",
    "                var_left = np.var(left_target)\n",
    "                var_right = np.var(right_target)\n",
    "\n",
    "                # Информативность\n",
    "                ginis[idx] = parent_criterion - (n_left / n_samples) * var_left - (n_right / n_samples) * var_right\n",
    "\n",
    "    # Находим лучший порог\n",
    "    valid_mask = ginis > -np.inf\n",
    "\n",
    "    if not np.any(valid_mask):\n",
    "        # Нет валидных разбиений\n",
    "        return np.array([]), np.array([]), np.inf, -np.inf\n",
    "\n",
    "    best_idx = np.argmax(ginis[valid_mask])\n",
    "    # Корректируем индекс с учётом фильтра\n",
    "    best_idx = np.where(valid_mask)[0][best_idx]\n",
    "\n",
    "    threshold_best = thresholds[best_idx]\n",
    "    gini_best = ginis[best_idx]\n",
    "\n",
    "    return thresholds, ginis, threshold_best, gini_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эту функцию можно протестировать на датасете `California`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data=data[\"data\"], columns=data[\"feature_names\"])\n",
    "y = data[\"target\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите график зависимости значения критерия ошибки от порогового значения при разбиении вершины по признаку `MedInc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите лучший, с вашей точки зрения, предикат первой вершины решающего дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.2 (1 балл)** Разберитесь с написанным кодом решающего дерева, заполните пропуски в коде и реализуйте недостающий метод `_predict_node()`.\n",
    "\n",
    "Построение дерева осуществляется согласно базовому жадному алгоритму, предложенному в лекции в разделе «Построение дерева».\n",
    "- **Выбор лучшего разбиения** необходимо производить по критерию Джини.\n",
    "- **Критерий останова:** все объекты в листе относятся к одному классу или ни по одному признаку нельзя разбить выборку.\n",
    "- **Ответ в листе:** наиболее часто встречающийся класс в листе.\n",
    "\n",
    "В задаче также предлагается получить два бонуса, по баллу на каждый!\n",
    "\n",
    "- **Реализуйте способ обрабатывать пропуски в даннх и реализуйте его, пояснив свои действия.**\n",
    "- **Реализуйте метод оценки важности признаков.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_types: Union[List[str], np.ndarray],\n",
    "        max_depth: int = None,\n",
    "        min_samples_split: int = None,\n",
    "        min_samples_leaf: int = None,\n",
    "        task: str = \"classification\"\n",
    "    ) -> None:\n",
    "\n",
    "        if np.any(list(map(lambda x: x != \"real\" and x != \"categorical\", feature_types))):\n",
    "            raise ValueError(\"There is unknown feature type\")\n",
    "\n",
    "        # В этой переменной будем хранить узлы решающего дерева. Каждая вершина хранит в себе идентификатор того,\n",
    "        # является ли она листовой. Листовые вершины хранят значение класса для предсказания, нелистовые - правого и\n",
    "        # левого детей (поддеревья для продолжения процедуры предсказания)\n",
    "        self._tree = {}\n",
    "\n",
    "        # типы признаков (категориальные или числовые)\n",
    "        self._feature_types = feature_types\n",
    "\n",
    "        # гиперпараметры дерева\n",
    "        self._max_depth = max_depth\n",
    "        self._min_samples_split = min_samples_split\n",
    "        self._min_samples_leaf = min_samples_leaf\n",
    "        self.task = task\n",
    "\n",
    "        # Переменная, если вы решите делать бонус\n",
    "        self._feature_importances = {}\n",
    "\n",
    "\n",
    "    def _fit_node(\n",
    "        self,\n",
    "        sub_X: np.ndarray,\n",
    "        sub_y: np.ndarray,\n",
    "        node: dict\n",
    "    ) -> None:\n",
    "\n",
    "        # критерий останова\n",
    "        if np.all(sub_y == sub_y[0]):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = sub_y[0]\n",
    "            return\n",
    "\n",
    "        feature_best, threshold_best, gini_best, split = None, None, None, None\n",
    "        for feature in range(sub_X.shape[1]):\n",
    "            feature_type = self._feature_types[feature]\n",
    "            categories_map = {}\n",
    "\n",
    "            # подготавливаем признак для поиска оптимального порога\n",
    "            if feature_type == \"real\":\n",
    "                feature_vector = sub_X[:, feature]\n",
    "            elif feature_type == \"categorical\":\n",
    "                # здесь могла быть реализация более сложного подхода к обработке категориального признака\n",
    "                feature_vector = sub_X[:, feature]\n",
    "\n",
    "            # ищем оптимальный порог\n",
    "            _, _, threshold, gini = find_best_split(feature_vector, sub_y, self.task, feature_type)\n",
    "\n",
    "            if gini_best is None or gini > gini_best:\n",
    "                feature_best = feature\n",
    "                gini_best = gini\n",
    "\n",
    "                # split - маска на объекты, которые должны попасть в левое поддерево\n",
    "                if feature_type == \"real\":\n",
    "                    threshold_best = threshold\n",
    "                    split = # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————\n",
    "                elif feature_type == \"categorical\":\n",
    "                    # в данной реализации это просто значение категории\n",
    "                    threshold_best = threshold\n",
    "                    split = # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "        # записываем полученные сплиты в атрибуты класса\n",
    "        if feature_best is None:\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        node[\"type\"] = \"nonterminal\"\n",
    "\n",
    "        node[\"feature_split\"] = feature_best\n",
    "        if self._feature_types[feature_best] == \"real\":\n",
    "            node[\"threshold\"] = threshold_best\n",
    "        elif self._feature_types[feature_best] == \"categorical\":\n",
    "            node[\"category_split\"] = threshold_best\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        node[\"left_child\"], node[\"right_child\"] = {}, {}\n",
    "        self._fit_node(sub_X[split], sub_y[split], node[\"left_child\"])\n",
    "        self._fit_node(sub_X[np.logical_not(split)], sub_y[np.logical_not(split)], node[\"right_child\"])\n",
    "\n",
    "    def _predict_node(self, x: np.ndarray, node: dict) -> int:\n",
    "        \"\"\"\n",
    "        Предсказание начинается с корневой вершины дерева и рекурсивно идёт в левое или правое поддерево в зависимости от значения\n",
    "        предиката на объекте. Листовая вершина возвращает предсказание.\n",
    "        :param x: np.array, элемент выборки\n",
    "        :param node: dict, вершина дерева\n",
    "        \"\"\"\n",
    "        # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self._fit_node(X, y, self._tree)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predicted = []\n",
    "        for x in X:\n",
    "            predicted.append(self._predict_node(x, self._tree))\n",
    "\n",
    "        return np.array(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.3 (1 балл)** Загрузите таблицу `students.csv` (это немного преобразованный датасет [User Knowledge](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)). В ней признаки объекта записаны в первых пяти столбцах, а в последнем записана целевая переменная (класс: 0 или 1). Постройте на одном изображении пять кривых \"порог — значение критерия Джини\" для всех пяти признаков. Отдельно визуализируйте диаграммы рассеяния \"значение признака — класс\" для всех пяти признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из кривых значений критерия Джини, по какому признаку нужно производить деление выборки на два поддерева? Согласуется ли этот результат с визуальной оценкой диаграмм рассеяиния? Как бы охарактеризовали вид кривой для \"хороших\" признаков, по которым выборка делится почти идеально? Чем отличаются кривые для признаков, по которым деление практически невозможно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.4 (1 балл)** Протестируйте свое решающее дерево на датасете [mushrooms](https://archive.ics.uci.edu/ml/datasets/Mushroom). \n",
    "\n",
    "1. Скачайте таблицу `agaricus-lepiota.data`, \n",
    "2. Считайте таблицу при помощи `pandas`,\n",
    "3. Примените к каждому столбцу `LabelEncoder` (из `sklearn`), чтобы преобразовать строковые имена категорий в натуральные числа. \n",
    "\n",
    "Первый столбец — это целевая переменная (e — edible, p — poisonous) Мы будем измерять качество с помощью accuracy, так что нам не очень важно, что будет классом 1, а что — классом 0. Обучите решающее дерево на половине случайно выбранных объектов (признаки в датасете категориальные) и сделайте предсказания для оставшейся половины. Вычислите accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: Бэггинг и случайный лес (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной части мы будем работать [с задачей предсказания диабета у пациента](https://www.kaggle.com/uciml/pima-indians-diabetes-database/data). Посмотрим на работу бэггинга над решающими деревьями и случайного леса, сравним их работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Outcome'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.1 (0.5 балла)** Разделите данные на признаки и целевую переменную. Разбейте датасет на обучающую и тестовую части в отношении 7:3. Затем разделите обучающую выборку на обучающую-обучающую и обучающую-валидационную в соотношении 7:3 (то есть в итоге должно получиться три выборки: обучающая-обучающая (0.49 от исходного датасета), обучающая-валидационная (0.21 от исходного датасета) и тестовая (0.3 от исходного датасета)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.2 (1 балл)** На обучающей-валидационной выборке подберите оптимальные значения гиперпараметров `max_depth` и `min_samples_leaf` для `DecisionTreeClassifier`. Для этого:\n",
    "1. Создайте списки с возможными значениями для перебора.\n",
    "2. Для каждой пары значений обучите дерево на обучающей-обучающей выборке и определите качество на обучающей-валидационной выборке. В качестве критерия будем использовать `f1-меру`.\n",
    "3. Выберите ту пару значений, которая даёт наилучшее качество на обучающей-валидационной выборке. \n",
    "\n",
    "\n",
    "Обучите решающее дерево с подобранными гиперпараметрами на **полной обучающей** выборке. Оцените качество классификации на тестовой выборке по метрикам `accuracy`, `precision` и `recall`, `auc_roc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.3 (0.5 балла)** Обучите [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) на 50 деревьях на **полной обучающей** выборке. Оцените качество классификации на тестовой выборке по тем же метрикам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.4 (1 балл)** Выполните кросс-валидацию на полной обучающей выборке и подберите оптимальные значения гиперпараметров `max_depth` и `min_samples_split` для `Random Forest` с 50 деревьями. Для этого:\n",
    "\n",
    "1. Создайте списки с возможными значениями для перебора.\n",
    "2. Для каждой пары значений проведите кросс-валидацию на полной обучающей выборке. Количество разбиений выберите на ваш вкус. В качестве критерия будем использовать `f1-меру`. Усредните значение критерия по всем прогонам кросс-валидации. \n",
    "3. Выберите ту пару значений, которая даёт наилучшее среднее качество. \n",
    "\n",
    "Обучите случайный лес с подобранными гиперпараметрами на **полной обучающей** выборке. Оцените качество классификации по тем же метрикам. Какая из трёх построенных моделей показала себя лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.5 (0.5 балла)** Постройте график зависимости AUC ROC на тестовой выборке от числа деревьев (`n_estimators`) для случайного леса, обучаемого на **полной обучающей** выборке. Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.6 (0.5 балла)** Для лучшей модели случайного леса из **Задания 3.4** посчитайте важность признаков и постройте bar plot. Какой признак оказался самым важным для определения диабета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
